# Chess endgames

In 1992, Gerald Tesauro shocked the world with TD-Gammon, a program that learned to play backgammon at a level close to that of the game's grandmasters. It used an artificial neural network (ANN) to evaluate game positions and was trained by self-play and the TD(λ) algorithm. Variants of the approach soon started being extended to chess, eventually reaching great success. This project aimed to build on their work by using an end-to-end approach to build a chess playing program reminiscent of TD-Gammon. Specifically, a multi layer perceptron (MLP) with random initial weights was trained by self-play with depth-limited minimax search and the TD(λ) algorithm in order to evaluate positions in two chess endgames. Also, due to a difference in learning difficulty between the two endgames, the transfer learning (TL) capabilities of the MLP from the easier to the harder endgame were investigated. The evaluation accuracy of the program was measured using the error between a game's true evaluation and the minimax evaluation. The program learned to evaluate the two endgames nearly perfectly while TL improved learning of the harder endgame in multiple ways.
